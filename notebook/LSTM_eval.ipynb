{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import  DataLoader\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from networks_debug import *\n",
    "from dataloader import WisdmDatasetParser, WisdmDataset\n",
    "\n",
    "from assistant import Assistant\n",
    "from stats import LearningStats\n",
    "from utils import *\n",
    "\n",
    "class model(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(model, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=250)\n",
    "        self.output = nn.Linear(250, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"x shape\", x.shape)\n",
    "        x = torch.swapaxes(x, 1,0)\n",
    "        x = torch.swapaxes(x, 2,0)\n",
    "        h0 = torch.zeros(1, x.size(1), 250)\n",
    "        c0 = torch.zeros(1, x.size(1), 250)\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "    \n",
    "#{\"lr\":0.0023,\"batch_size\":256.0,\"hidden_size\":250.0}\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.model = model(input_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.model(input)\n",
    "        return output\n",
    "    \n",
    "\n",
    "class loss_wrapper():\n",
    "    def __init__(self):\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def __call__(self, output, target):\n",
    "        return self.loss(output.sum(0), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,)\n",
      "(6,)\n",
      "ytrain shape (21720, 7)\n",
      "yval shape (7240, 7)\n",
      "ytest shape (7241, 7)\n",
      "num classes train dataset: 7 occurrences of each class:[3189 2987 3083 3262 3046 3071 3082]\n",
      "num classes eval dataset: 7 occurrences of each class:[1050 1017  982  998 1058 1055 1080]\n",
      "num classes test dataset: 7 occurrences of each class:[1031  948 1014 1076 1062 1038 1072]\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME = 'watch_subset2_40_Ben.npz'\n",
    "DATASET_SUBSET = None\n",
    "NUM_WORKERS = 8\n",
    "batch_size = 256\n",
    "\n",
    "dataset = WisdmDatasetParser(f'{Path.home()}/snntorch_network/data/{DATASET_NAME}', norm=None, class_sublset=DATASET_SUBSET)\n",
    "train_set = dataset.get_training_set()\n",
    "val_set = dataset.get_validation_set()\n",
    "test_set = dataset.get_test_set()\n",
    "\n",
    "train_dataset = WisdmDataset(train_set)\n",
    "val_dataset = WisdmDataset(val_set)\n",
    "test_dataset = WisdmDataset(test_set)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=int(batch_size), shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader  = DataLoader(dataset= val_dataset, batch_size=int(batch_size), shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=int(batch_size), shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm_dataloader = tqdm(test_loader)\n",
    "# loss = 0\n",
    "# item_count = 0\n",
    "# total_correct = 0\n",
    "# for _, batch in enumerate(tqdm_dataloader): #eval loop\n",
    "#     with torch.no_grad():\n",
    "#         input, label = batch\n",
    "#         #print(\"input shape\", input)\n",
    "#         #print(\"input shape\", input.shape)\n",
    "#         output = net(input)\n",
    "#         #print(\"output shape\", output)\n",
    "#         batch_loss = loss_fn(output.sum(0), label)\n",
    "#         loss += batch_loss * len(label)\n",
    "#         item_count += len(label)\n",
    "#         _, idx = output.sum(dim=0).max(1)\n",
    "#         # print(\"idx shape\", idx)\n",
    "#         # print(\"label shape\", label)\n",
    "#         batch_accuracy = (label == idx).sum() / len(label)\n",
    "#         total_correct += (label == idx).sum()\n",
    "#         tqdm_dataloader.set_description(f'\\r Testing results: Loss: {loss / item_count:.4f}, Accuracy{total_correct / item_count:.4f}, BATCH ACCURACY: {batch_accuracy:.4f} batch_loss: {batch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'data_watch_subset_0_40.npz'\n",
    "DATASET_SUBSET = 'custom'\n",
    "SUBSET_LIST = [0, 2, 6, 8, 9, 14, 17]\n",
    "PATIENCE = 12\n",
    "TRAIN_FOLDER_NAME = 'Trained'\n",
    "NUM_WORKERS = 8\n",
    "NET_OUTPUT_DIM = 7\n",
    "NET_INPUT_DIM = 6\n",
    "NUM_EPOCHS = 100\n",
    "SEARCH_SPACE_SHUFFLE = 200\n",
    "trained_folder = TRAIN_FOLDER_NAME\n",
    "os.makedirs(trained_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = LSTMNet(6, 7)\n",
    "# checkpoint = torch.load(\"LSTM.pt\")\n",
    "# print(checkpoint.keys())\n",
    "# #net.load_state_dict(checkpoint)\n",
    "\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=0.0023, betas=(0.9, 0.999))\n",
    "\n",
    "# loss_fn = loss_wrapper()\n",
    "# stats = LearningStats()\n",
    "# assistant = Assistant(net, loss_fn, optimizer, stats, classifier=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsets_list = {\n",
    "#     'best_Marco':[1, 6, 7, 8, 13, 14, 17],\n",
    "#     'my_best':[0, 2, 6, 8, 9, 14, 17],\n",
    "#     'second_best':[0, 4, 6, 8, 9, 10, 14],\n",
    "#     'best_mse':[4, 5, 8, 9, 11, 14, 17],\n",
    "#     'subset_2':[6, 7, 8, 9, 10, 11, 12],\n",
    "#     'balanced':[0, 1, 4, 8, 9, 10, 14]\n",
    "# }\n",
    "subsets_list = {\n",
    "\n",
    "    'best_mse':[4, 5, 8, 9, 11, 14, 17]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on best_mse, with subset [4, 5, 8, 9, 11, 14, 17]\n",
      "(6,)\n",
      "(6,)\n",
      "ytrain shape (55404, 18)\n",
      "yval shape (18468, 18)\n",
      "ytest shape (18469, 18)\n",
      "num classes train dataset: 7 occurrences of each class:[3044 3092 3047 3150 3058 2973 3213]\n",
      "num classes eval dataset: 7 occurrences of each class:[1048 1038  996 1110 1039 1007 1077]\n",
      "num classes test dataset: 7 occurrences of each class:[1048 1029 1036 1076 1067  982 1054]\n",
      "odict_keys(['model.lstm.weight_ih_l0', 'model.lstm.weight_hh_l0', 'model.lstm.bias_ih_l0', 'model.lstm.bias_hh_l0', 'model.output.weight', 'model.output.bias'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch  0/100] Training: loss =     2.03957                          accuracy = 0.47480 :  49%|████▉     | 42/85 [00:04<00:04,  9.31it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm_dataloader): \u001b[38;5;66;03m# training loop\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28minput\u001b[39m, label \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 42\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43massistant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     tqdm_dataloader\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Training: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats\u001b[38;5;241m.\u001b[39mtraining\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, refresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     45\u001b[0m tqdm_dataloader \u001b[38;5;241m=\u001b[39m tqdm(val_loader)\n",
      "File \u001b[0;32m~/snntorch_network/notebook/../src/assistant.py:137\u001b[0m, in \u001b[0;36mAssistant.train\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mtraining\u001b[38;5;241m.\u001b[39mcorrect_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\n\u001b[1;32m    132\u001b[0m             idx \u001b[38;5;241m==\u001b[39m target\n\u001b[1;32m    133\u001b[0m         )\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 137\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for name, subset in subsets_list.items():\n",
    "    DATASET_NAME = 'data_watch_subset_0_40.npz'\n",
    "    DATASET_SUBSET = 'custom'\n",
    "    SUBSET_LIST = subset\n",
    "    NUM_WORKERS = 8\n",
    "    batch_size = 256\n",
    "    print(f'Working on {name}, with subset {subset}')\n",
    "    dataset = WisdmDatasetParser(f'{Path.home()}/snntorch_network/data/{DATASET_NAME}', norm=None, class_sublset=DATASET_SUBSET, subset_list=SUBSET_LIST)\n",
    "    train_set = dataset.get_training_set()\n",
    "    val_set = dataset.get_validation_set()\n",
    "\n",
    "    train_dataset = WisdmDataset(train_set)\n",
    "    val_dataset = WisdmDataset(val_set)\n",
    "    test_dataset = WisdmDataset(test_set)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=int(batch_size), shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_loader  = DataLoader(dataset= val_dataset, batch_size=int(batch_size), shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    trained_folder = 'LSTM-benchmark'+'/'+name\n",
    "    os.makedirs(trained_folder, exist_ok=True)\n",
    "    net = LSTMNet(6, 7)\n",
    "    checkpoint = torch.load(\"LSTM.pt\")\n",
    "    print(checkpoint.keys())\n",
    "    #net.load_state_dict(checkpoint)\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.0023, betas=(0.9, 0.999))\n",
    "\n",
    "    loss_fn = loss_wrapper()\n",
    "    stats = LearningStats()\n",
    "    assistant = Assistant(net, loss_fn, optimizer, stats, classifier=True)\n",
    "    count = 0\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        labels = []\n",
    "        outputs = []\n",
    "        # if epoch % 20 == 0:\n",
    "        #     assistant.reduce_lr()\n",
    "        if count < PATIENCE:\n",
    "            count = count+1\n",
    "            tqdm_dataloader = tqdm(train_loader)\n",
    "            for _, batch in enumerate(tqdm_dataloader): # training loop\n",
    "                input, label = batch\n",
    "                output = assistant.train(input, label)\n",
    "                tqdm_dataloader.set_description(f'\\r[Epoch {epoch:2d}/{NUM_EPOCHS}] Training: {stats.training}', refresh=False)\n",
    "\n",
    "            tqdm_dataloader = tqdm(val_loader)\n",
    "            for _, batch in enumerate(tqdm_dataloader): #eval loop\n",
    "                input, label = batch\n",
    "                output = assistant.test(input, label)\n",
    "                tqdm_dataloader.set_description(f'\\r[Epoch {epoch:2d}/{NUM_EPOCHS}] Validation: {stats.testing}')\n",
    "        \n",
    "                if len(outputs) == 0:\n",
    "                    outputs = output.to('cpu').detach()\n",
    "                    labels = label.to('cpu').detach()\n",
    "                else:\n",
    "                    outputs = torch.cat((outputs, output.to('cpu').detach()), dim=1)\n",
    "                    labels = torch.cat((labels, label.to('cpu').detach()))\n",
    "            \n",
    "            stats.update()\n",
    "\n",
    "            if stats.testing.best_accuracy:\n",
    "                count = 0\n",
    "                _, predictions = outputs.sum(dim=0).max(1)\n",
    "                gen_confusion_matrix(predictions,labels, f'./{trained_folder}/')\n",
    "                del predictions\n",
    "            del labels\n",
    "            del outputs\n",
    "            stats.save( f'./{trained_folder}/')\n",
    "            stats.plot(path=f'./{trained_folder}/')\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "        clear_output(wait=True)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
