{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "from snntorch import utils\n",
    "from snntorch import surrogate\n",
    "import torch.nn.functional as F\n",
    "from snntorch import functional as SF\n",
    "import brevitas.nn as qnn \n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from my_network import *\n",
    "from dataloader import WisdmDatasetParser, WisdmDataset\n",
    "from torch.utils.data import  DataLoader\n",
    "from assistant import Assistant\n",
    "from stats import LearningStats\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaky1_betas: 1.01234897878021\n",
      "leaky1_vth: 2.195924758911133\n",
      "leaky2_betas: 0.07083219289779663\n",
      "leaky2_vth: 0.8788766860961914\n",
      "recurrent_leaky_betas: 0.11015737056732178\n",
      "recurrent_vth: 1.8151839971542358\n",
      "leaky3_betas: 0.0\n",
      "leaky3_vth: 0.830508828163147\n",
      "Zero counts in matrices: [0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#path = f\"{Path.home()}/snntorch_network/nni_experiments/inibitory_lif_no_encoder_best/results/ot6eqima/trials/oqxoS/Trained/network_best.npz\"\n",
    "#path = f\"{Path.home()}/snntorch_network/nni_experiments/inibitory_lif_no_encoder_worst/results/ipk2erm5/trials/whXvC/Trained/network_best.npz\"\n",
    "path = f\"{Path.home()}/snntorch_network/nni_experiments/inibitory_lif_no_encoder_balanced/results/4m8j0yfa/trials/yAnF7/Trained/network_best.npz\"\n",
    "#path = f\"{Path.home()}/snntorch_network/nni_experiments/inibitory_lif_no_encoder_balanced_no_net_loss/results/uhrgfo1n/trials/TbZxT/Trained/network_best.npz\" # best accuracy\n",
    "#path = f\"{Path.home()}/snntorch_network/nni_experiments/inibitory_lif_no_encoder_balanced_no_net_loss/results/uhrgfo1n/trials/bnh2o/Trained/network_best.npz\" #same HP as the one with net_loss\n",
    "\n",
    "name = \"balanced_with_loss\"\n",
    "data = np.load(path,allow_pickle=True)\n",
    "\n",
    "linear1_w= data['linear1']\n",
    "leaky1_vth= data['leaky1_vth']\n",
    "leaky1_betas= 1-data['leaky1_betas'] \n",
    "leaky1_betas= leaky1_betas if leaky1_betas >= 0 else np.zeros(leaky1_betas.shape)\n",
    "print(f\"leaky1_betas: {leaky1_betas}\")\n",
    "print(f\"leaky1_vth: {leaky1_vth}\")\n",
    "linear2_w = data['linear2']\n",
    "leaky2_vth= data['recurrent_vth']\n",
    "leaky2_betas= 1 - data['recurrent_betas']\n",
    "leaky2_betas= leaky2_betas if  leaky2_betas >= 0 else np.zeros(leaky2_betas.shape)\n",
    "print(f\"leaky2_betas: {leaky2_betas}\")\n",
    "print(f\"leaky2_vth: {leaky2_vth}\")\n",
    "\n",
    "recurrent_in_weights = data['input_dense']\n",
    "recurrent_out_weights = - data['output_dense']\n",
    "recurrent_vth = data['activation_vth']\n",
    "recurrent_leaky_betas = 1 - data['activation_betas']\n",
    "recurrent_leaky_betas= recurrent_leaky_betas if recurrent_leaky_betas >= 0 else np.zeros(recurrent_leaky_betas.shape)\n",
    "print(f\"recurrent_leaky_betas: {recurrent_leaky_betas}\")\n",
    "print(f\"recurrent_vth: {recurrent_vth}\")\n",
    "\n",
    "linear3_w = data['linear3']\n",
    "leaky3_vth= data['leaky2_vth']\n",
    "leaky3_betas= 1 - data['leaky2_betas']\n",
    "leaky3_betas= leaky3_betas if leaky3_betas >= 0 else np.zeros(leaky3_betas.shape)\n",
    "print(f\"leaky3_betas: {leaky3_betas}\")\n",
    "print(f\"leaky3_vth: {leaky3_vth}\")\n",
    "\n",
    "# Given a list of numpy matrices, count the zeros inside\n",
    "def count_zeros(matrix_list):\n",
    "    return [np.count_nonzero(matrix == 0) for matrix in matrix_list]\n",
    "\n",
    "# Example usage\n",
    "matrix_list = [linear1_w, linear2_w, linear3_w, recurrent_in_weights, recurrent_out_weights]\n",
    "zero_counts = count_zeros(matrix_list)\n",
    "print(f\"Zero counts in matrices: {zero_counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 604700\n",
      "model footprint int 8: 590.52734375 KB\n",
      "model footprint fp 16: 2362.109375 KB\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(matrix_list):\n",
    "    return sum(matrix.size for matrix in matrix_list)\n",
    "\n",
    "# Example usage\n",
    "matrix_list = [linear1_w, linear2_w, linear3_w, recurrent_in_weights, recurrent_out_weights]\n",
    "total_parameters = count_parameters(matrix_list)\n",
    "print(f\"Total number of parameters: {total_parameters}\")\n",
    "print(f\"model footprint int 8: {total_parameters/1024} KB\")\n",
    "print(f\"model footprint fp 16: {total_parameters*4/1024} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "device = 'cuda'\n",
    "print(f'Using device {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "slope = 10\n",
    "# network parameters\n",
    "num_inputs = 6 \n",
    "num_steps = 40\n",
    "net_hidden_1 = 180\n",
    "net_hidden_2 = 400\n",
    "net_hidden_3 = 128\n",
    "num_outputs = 7\n",
    "pop_outputs = num_outputs * 10\n",
    "num_epochs = 200\n",
    "vth_in = 1.0\n",
    "vth_out = 1.0\n",
    "vth_recurrent = 1.0\n",
    "vth_enc_value =  1.0\n",
    "vth_std =  65 \n",
    "beta_in = 0.5\n",
    "beta_recurrent = 0.5\n",
    "beta_back = 0.6\n",
    "beta_out = 0.5\n",
    "encoder_dim = 25\n",
    "beta_std = 55\n",
    "lr = 0.002\n",
    "drop_recurrent =0.15\n",
    "drop_back = 0.15\n",
    "drop_out = 0.15\n",
    "# spiking neuron parameters\n",
    "beta = 0.8  # neuron decay rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'data_watch_subset_0_40.npz'\n",
    "DATASET_SUBSET = 'custom'\n",
    "PATIENCE = 12\n",
    "TRAIN_FOLDER_NAME = 'Trained'\n",
    "NUM_WORKERS = 8\n",
    "NET_OUTPUT_DIM = 7\n",
    "NET_INPUT_DIM = 6\n",
    "NUM_EPOCHS = 200\n",
    "SEARCH_SPACE_SHUFFLE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,)\n",
      "(6,)\n",
      "ytrain shape (55404, 18)\n",
      "yval shape (18468, 18)\n",
      "ytest shape (18469, 18)\n",
      "num classes train dataset: 7 occurrences of each class:[3127 3066 3044 3047 3150 3087 2973]\n",
      "num classes eval dataset: 7 occurrences of each class:[1035  968 1048  996 1110 1053 1007]\n",
      "num classes test dataset: 7 occurrences of each class:[1046 1061 1048 1036 1076 1026  982]\n",
      "val dataset shape: (7217, 6, 40)\n",
      "Using device cuda\n",
      "type of self.linear2 is <class 'brevitas.nn.quant_linear.QuantLinear'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/franzhd/miniconda3/envs/snn_torch/lib/python3.10/site-packages/brevitas/nn/mixin/base.py:77: UserWarning: Keyword arguments are being passed but they not being used.\n",
      "  warn('Keyword arguments are being passed but they not being used.')\n"
     ]
    }
   ],
   "source": [
    "SUBSET_LIST = [0, 1, 4, 8, 9, 10, 14]\n",
    "trained_folder = TRAIN_FOLDER_NAME\n",
    "os.makedirs(trained_folder, exist_ok=True)\n",
    "dataset = WisdmDatasetParser(f'{Path.home()}/snntorch_network/data/{DATASET_NAME}', norm=None, class_sublset=DATASET_SUBSET, subset_list=SUBSET_LIST)\n",
    "val_set = dataset.get_validation_set(shuffle=False, subset=None)\n",
    "print(f\"val dataset shape: {val_set[0].shape}\")\n",
    "# val_set = (np.transpose(val_set[0], (0, 2, 1)), one_hot_encode(val_set[1],7))\n",
    "val_dataset = WisdmDataset(val_set)\n",
    "# print(f\"val dataset shape: {val_set[0].shape}\")\n",
    "# print(f\"val dataset shape: {val_set[1].shape}\")\n",
    "val_loader  = DataLoader(dataset= val_dataset, batch_size=int(batch_size), shuffle=True, num_workers=NUM_WORKERS, drop_last=False)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "# device = 'cpu'\n",
    "print(f'Using device {device}')\n",
    "\n",
    "grad = surrogate.fast_sigmoid(slope) #use slope for HPO\n",
    "stats = LearningStats()\n",
    "\n",
    "net_loss = regularization_loss(0.1, 0.03, 40)\n",
    "\n",
    "net = ExInbitoryNetwork(NET_INPUT_DIM, 200,500, NET_OUTPUT_DIM, grad,\n",
    "                    vth_in=vth_in, vth_recurrent=vth_recurrent, vth_out=vth_out, vth_back=1.0,\n",
    "                    beta_in=beta_in, beta_recurrent=beta_recurrent, beta_back=beta_back, beta_out=beta_out,\n",
    "                    # encoder_dim=int(encoder_dim),\n",
    "                    # vth_enc_value=vth_enc_value, vth_std=vth_std, beta_std=beta_std,\n",
    "                    drop_recurrent=drop_recurrent, drop_back=drop_back, drop_out=drop_out,\n",
    "                    time_dim=2).to(device)\n",
    "\n",
    "net.from_npz(path)\n",
    "\n",
    "net.to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), 0.01, betas=(0.9, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "        T_max=4690, \n",
    "        eta_min=0, \n",
    "        last_epoch=-1\n",
    "    )\n",
    "\n",
    "\n",
    "loss_fn = SF.loss.ce_count_loss()\n",
    "\n",
    "assistant = Assistant(net, loss_fn, optimizer, stats, classifier=True, scheduler=scheduler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29 [00:00<?, ?it/s]/home/franzhd/miniconda3/envs/snn_torch/lib/python3.10/site-packages/torch/_tensor.py:1488: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1928.)\n",
      "  return super().rename(names)\n",
      "/home/franzhd/miniconda3/envs/snn_torch/lib/python3.10/site-packages/snntorch/_neurons/leaky.py:211: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:311.)\n",
      "  self.mem = torch.zeros_like(input_, device=self.mem.device)\n",
      " Validation: loss =     0.27931 (min =     0.04009)     accuracy = 0.93878 (max = 0.99219) : 100%|██████████| 29/29 [00:06<00:00,  4.20it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tqdm_dataloader = tqdm(val_loader)\n",
    "for _, batch in enumerate(tqdm_dataloader): #eval loop\n",
    "        input, label = batch\n",
    "        output = assistant.valid(input, label)\n",
    "        tqdm_dataloader.set_description(f'\\r Validation: {stats.validation}')\n",
    "    \n",
    "\n",
    "        stats.update()\n",
    "\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neurobench.models import SNNTorchModel\n",
    "# from neurobench.postprocessing.postprocessor import aggregate, choose_max_count\n",
    "# from neurobench.benchmarks import Benchmark\n",
    "# import neurobench.benchmarks.static_metrics\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# model = SNNTorchModel(net)\n",
    "# postprocessors = [choose_max_count]\n",
    "# for param in model.__net__().parameters():\n",
    "#     print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# static_metrics = [\"footprint\",\"parameter_count\"]\n",
    "# workload_metrics = [\"activation_sparsity\", \"membrane_updates\", \"synaptic_operations\"]\n",
    "\n",
    "# benchmark = Benchmark(model, val_loader, [], postprocessors, [static_metrics, workload_metrics])\n",
    "# results = benchmark.run(device=device)\n",
    "# print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Define the path to save the JSON file\n",
    "# results_path = f'{name}.json'\n",
    "\n",
    "# # Save the results dictionary to a JSON file\n",
    "# with open(results_path, 'w') as json_file:\n",
    "#     json.dump(results, json_file)\n",
    "\n",
    "# print(f\"Results saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------------------------------------------------------+--------------------+-------------------------------------------------------------+\n",
      "| Metric              | original                                                    | no_loss_optimal    | original_no_loss                                            |\n",
      "+=====================+=============================================================+====================+=============================================================+\n",
      "| footprint           | 2419012                                                     | 2419012            | 630092                                                      |\n",
      "+---------------------+-------------------------------------------------------------+--------------------+-------------------------------------------------------------+\n",
      "| parameter_count     | 604710                                                      | 604710             | 157480                                                      |\n",
      "+---------------------+-------------------------------------------------------------+--------------------+-------------------------------------------------------------+\n",
      "| activation_sparsity | 0.9423620933521923                                          | 0.7445445721357843 | 0.8637645557029172                                          |\n",
      "+---------------------+-------------------------------------------------------------+--------------------+-------------------------------------------------------------+\n",
      "| membrane_updates    | 7353.5                                                      | 6807.251           | 3575.9675                                                   |\n",
      "+---------------------+-------------------------------------------------------------+--------------------+-------------------------------------------------------------+\n",
      "| synaptic_operations | {'Effective_MACs': 0.0, 'Effective_ACs': 0.0, 'Dense': 0.0} | N/A                | {'Effective_MACs': 0.0, 'Effective_ACs': 0.0, 'Dense': 0.0} |\n",
      "+---------------------+-------------------------------------------------------------+--------------------+-------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load the JSON files\n",
    "with open('balanced_with_loss.json', 'r') as f1, open('balanced_no_net_loss_big.json', 'r') as f2, open('balanced_no_net_loss_small.json', 'r') as f3:\n",
    "    data1 = json.load(f1)\n",
    "    data2 = json.load(f2)\n",
    "    data3 = json.load(f3)\n",
    "\n",
    "# Prepare data for tabulate\n",
    "table_data = []\n",
    "for key in data1.keys():\n",
    "    value1 = data1.get(key, 'N/A')\n",
    "    value2 = data2.get(key, 'N/A')\n",
    "    value3 = data3.get(key, 'N/A')\n",
    "    table_data.append([key, value1, value2, value3])\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=[\"Metric\", \"original\", \"no_loss_optimal\", \"original_no_loss\"], tablefmt=\"grid\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
